{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1135</td><td>application_1511276242554_0472</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1511276242554_0472/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop26:8042/node/containerlogs/container_e25_1511276242554_0472_01_000001/bonus_lab2__diegorc0\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "def wrapper(learning_rate, dropout):\n",
    "\n",
    "\timport tensorflow as tf\n",
    "\timport numpy as np\n",
    "\tfrom hops import tensorboard\n",
    "\tfrom hops import hdfs\n",
    "\n",
    "\t# Training Parameters\n",
    "\t#learning_rate = 0.001\n",
    "\tnum_steps = 200\n",
    "\tbatch_size = 128\n",
    "\n",
    "\t# Network Parameters\n",
    "\tnum_input = 784 # MNIST data input (img shape: 28*28)\n",
    "\tnum_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\ttrain_filenames = [hdfs.project_path() + \"mnist/train.tfrecords\"]\n",
    "\tvalidation_filenames = [hdfs.project_path() + \"mnist/validation.tfrecords\"]\n",
    "\n",
    "\t# Create the neural network\n",
    "\tdef conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "\n",
    "\t    # Define a scope for reusing the variables\n",
    "\t    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "# TF Estimator input is a dict, in case of multiple inputs\n",
    "\t\tx = x_dict\n",
    "\n",
    "# MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "# Reshape to match picture format [Height x Width x Channel]\n",
    "# Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "\t\tx = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "        #Gets an existing variable with these parameters or create a new one under some variable_scope\n",
    "\t\tW1 = tf.get_variable('W1',initializer=tf.truncated_normal([5, 5, 1, 4], stddev=0.1))\n",
    "\t\tB1 = tf.get_variable('B1',initializer=tf.zeros([4]))\n",
    "\t\tW2 = tf.get_variable('W2',initializer=tf.truncated_normal([5, 5, 4, 8], stddev=0.1))\n",
    "\t\tB2 = tf.get_variable('B2',initializer=tf.zeros([8]))\n",
    "\t\tW3 = tf.get_variable('W3',initializer=tf.truncated_normal([4, 4, 8, 12], stddev=0.1))\n",
    "\t\tB3 = tf.get_variable('B3',initializer=tf.zeros([12]))\n",
    "\t\tW4 = tf.get_variable('W4',initializer=tf.truncated_normal([588, 200], stddev=0.1))\n",
    "\t\tB4 = tf.get_variable('B4',initializer=tf.zeros([200]))\n",
    "\t\tW5 = tf.get_variable('W5',initializer=tf.truncated_normal([200, 10], stddev=0.1))\n",
    "\t\tB5 = tf.get_variable('B5',initializer=tf.zeros([10]))\n",
    "\n",
    "\n",
    "# 2. Define the model\n",
    "#Note that to define a single value placeholder, a scalar, shape is () or [], not 0\n",
    "\t\tpkeep = 1\n",
    "\t\tif is_training == True:\n",
    "\t\t\tpkeep = dropout\n",
    "\t\tY1 = tf.nn.relu(tf.nn.conv2d(x, W1, strides=[1,1,1,1], padding=\"SAME\")+B1)\n",
    "\t\tY1d = tf.nn.dropout(Y1, pkeep)\n",
    "#as strides every 2x2 steps, only half of the values of the previous output 28x28 are kept: 14x14\n",
    "\t\tY2 = tf.nn.relu(tf.nn.conv2d(Y1d, W2, strides=[1,2,2,1],padding=\"SAME\")+B2)\n",
    "\t\tY2d = tf.nn.dropout(Y2, pkeep)\n",
    "\t\tY3 = tf.nn.relu(tf.nn.conv2d(Y2d, W3, strides=[1,2,2,1],padding=\"SAME\")+B3)\n",
    "\t\tY3d = tf.nn.dropout(Y3, pkeep)\n",
    "#588=7x7x12 (12 output channels)\n",
    "\t\tY3d_reshape = tf.reshape(Y3d, shape=[-1,588])\n",
    "\t\tY4 = tf.nn.relu(tf.matmul(Y3d_reshape, W4) + B4)\n",
    "\t\tY4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "\t\tYlogits = tf.matmul(Y4d, W5) + B5\n",
    "\n",
    "\t    return Ylogits\n",
    "    \n",
    "\n",
    "\n",
    "\t# Define the model function (following TF Estimator Template)\n",
    "\tdef model_fn(features, labels, mode, params):\n",
    "\n",
    "\t    # Build the neural network\n",
    "\t    # Because Dropout have different behavior at training and prediction time, we\n",
    "\t    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "\t    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "\t    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "\n",
    "\n",
    "\t    # Predictions\n",
    "\t    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "\t    pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "\t    # If prediction mode, early return\n",
    "\t    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "\t    # Define loss and optimizer\n",
    "\t    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "\t\tlogits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "\t    lr = tf.train.exponential_decay(learning_rate, tf.train.get_global_step(),100000,0.96)\n",
    "\t    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\t    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "\t    # Evaluate the accuracy of the model\n",
    "\t    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "\t    image = tf.reshape(features[:10], [-1, 28, 28, 1])\n",
    "\t    tf.summary.image(\"image\", image)\n",
    "\t    # tf.summary.scalar('my_accuracy', acc_op[0])\n",
    "\n",
    "\t    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "\t    # the different ops for training, evaluating, ...\n",
    "\t    estim_specs = tf.estimator.EstimatorSpec(\n",
    "\t      mode=mode,\n",
    "\t      predictions=pred_classes,\n",
    "\t      loss=loss_op,\n",
    "\t      train_op=train_op,\n",
    "\t      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "\t    return estim_specs\n",
    "\n",
    "\n",
    "\tdef data_input_fn(filenames, batch_size=128, shuffle=False, repeat=None):\n",
    "\n",
    "\t    def parser(serialized_example):\n",
    "\t\t\"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "\t\tfeatures = tf.parse_single_example(\n",
    "\t\t    serialized_example,\n",
    "\t\t    features={\n",
    "\t\t        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "\t\t        'label': tf.FixedLenFeature([], tf.int64),\n",
    "\t\t    })\n",
    "\t\timage = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "\t\timage.set_shape([28 * 28])\n",
    "\n",
    "\t\t# Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "\t\timage = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "\t\tlabel = tf.cast(features['label'], tf.int32)\n",
    "\t\treturn image, label\n",
    "\n",
    "\t    def _input_fn():\n",
    "\t\t# Import MNIST data\n",
    "\t\tdataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "\t\t# Map the parser over dataset, and batch results by up to batch_size\n",
    "\t\tdataset = dataset.map(parser, num_threads=1, output_buffer_size=batch_size)\n",
    "\t\tif shuffle:\n",
    "\t\t    dataset = dataset.shuffle(buffer_size=128)\n",
    "\t\tdataset = dataset.batch(batch_size)\n",
    "\t\tdataset = dataset.repeat(repeat)\n",
    "\t\titerator = dataset.make_one_shot_iterator()\n",
    "\n",
    "\t\tfeatures, labels = iterator.get_next()\n",
    "\n",
    "\t\treturn features, labels\n",
    "\n",
    "\t    return _input_fn\n",
    "\n",
    "\n",
    "\trun_config = tf.contrib.learn.RunConfig(\n",
    "\t    model_dir=tensorboard.logdir(),\n",
    "\t    save_checkpoints_steps=10,\n",
    "\t    save_summary_steps=5,\n",
    "\t    log_step_count_steps=10)\n",
    "\n",
    "\thparams = tf.contrib.training.HParams(\n",
    "        learning_rate=learning_rate, dropout_rate=dropout)\n",
    "\n",
    "\tsummary_hook = tf.train.SummarySaverHook(\n",
    "\t      save_steps = run_config.save_summary_steps,\n",
    "\t      scaffold= tf.train.Scaffold(),\n",
    "\t      summary_op=tf.summary.merge_all())\n",
    "\n",
    "\tmnist_estimator = tf.estimator.Estimator(\n",
    "\t    model_fn=model_fn,\n",
    "\t    config=run_config,\n",
    "\t    params=hparams\n",
    "\t)\n",
    "\n",
    "\n",
    "\ttrain_input_fn = data_input_fn(train_filenames[0], batch_size=batch_size)\n",
    "\teval_input_fn = data_input_fn(validation_filenames[0], batch_size=batch_size)\n",
    "\n",
    "\texperiment = tf.contrib.learn.Experiment(\n",
    "\t    mnist_estimator,\n",
    "\t    train_input_fn=train_input_fn,\n",
    "\t    eval_input_fn=eval_input_fn,\n",
    "\t    train_steps=num_steps,\n",
    "\t    min_eval_frequency=5,\n",
    "\t    eval_hooks=[summary_hook]\n",
    "\t)\n",
    "\n",
    "\texperiment.train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.002, 0.002, 0.002, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005], 'dropout': [0.3, 0.6, 0.75, 0.3, 0.6, 0.75, 0.3, 0.6, 0.75]}"
     ]
    }
   ],
   "source": [
    "from hops import util\n",
    "\n",
    "#Define dict for hyperparameters\n",
    "args_dict = {'learning_rate': [0.002,0.001,0.0005], 'dropout': [0.3,0.6,0.75]}\n",
    "\n",
    "# Generate a grid for the given hyperparameters\n",
    "args_dict_grid = util.grid_params(args_dict)\n",
    "\n",
    "print(args_dict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-29T00:06:41.531667 ===== Start\n",
      "Finished TensorFlow job \n",
      "\n",
      "Make sure to check /Logs/TensorFlow/application_1511276242554_0472/runId.0 for logfile and TensorBoard logdir\n",
      "Elapsed time: 82.5034301281\n",
      "2017-11-29T00:08:04.035736 ===== Stop"
     ]
    }
   ],
   "source": [
    "from hops import tflauncher\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "print(\"{0} ===== Start\".format(datetime.now().isoformat()))\n",
    "start_time = timeit.default_timer()\n",
    "tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper, args_dict_grid)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print \"Elapsed time: \" + str(elapsed)\n",
    "print(\"{0} ===== Stop\".format(datetime.now().isoformat()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Optimal parameters: 0.002 and 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job 2 cancelled \n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1704)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/bonus_lab2/lib/python2.7/site-packages/hops/tensorboard.py\", line 118, in visualize\n",
      "    nodeRDD.foreachPartition(start_visualization(hdfs_root_logdir, app_id))\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/ur0VAaNk9QbnIdB3WdQEnlZ8aoq9ZZGRXTdAINL26Lg/appcache/application_1511276242554_0472/container_e25_1511276242554_0472_01_000001/pyspark.zip/pyspark/rdd.py\", line 799, in foreachPartition\n",
      "    self.mapPartitions(func).count()  # Force evaluation\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/ur0VAaNk9QbnIdB3WdQEnlZ8aoq9ZZGRXTdAINL26Lg/appcache/application_1511276242554_0472/container_e25_1511276242554_0472_01_000001/pyspark.zip/pyspark/rdd.py\", line 1041, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/ur0VAaNk9QbnIdB3WdQEnlZ8aoq9ZZGRXTdAINL26Lg/appcache/application_1511276242554_0472/container_e25_1511276242554_0472_01_000001/pyspark.zip/pyspark/rdd.py\", line 1032, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/ur0VAaNk9QbnIdB3WdQEnlZ8aoq9ZZGRXTdAINL26Lg/appcache/application_1511276242554_0472/container_e25_1511276242554_0472_01_000001/pyspark.zip/pyspark/rdd.py\", line 906, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/ur0VAaNk9QbnIdB3WdQEnlZ8aoq9ZZGRXTdAINL26Lg/appcache/application_1511276242554_0472/container_e25_1511276242554_0472_01_000001/pyspark.zip/pyspark/rdd.py\", line 809, in collect\n",
      "    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/ur0VAaNk9QbnIdB3WdQEnlZ8aoq9ZZGRXTdAINL26Lg/appcache/application_1511276242554_0472/container_e25_1511276242554_0472_01_000001/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/ur0VAaNk9QbnIdB3WdQEnlZ8aoq9ZZGRXTdAINL26Lg/appcache/application_1511276242554_0472/container_e25_1511276242554_0472_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/ur0VAaNk9QbnIdB3WdQEnlZ8aoq9ZZGRXTdAINL26Lg/appcache/application_1511276242554_0472/container_e25_1511276242554_0472_01_000001/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job 2 cancelled \n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1704)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hops import tensorboard\n",
    "\n",
    "# Visualize all TensorBoard events for the jobs in the same TensorBoard\n",
    "tensorboard.visualize(spark, tensorboard_hdfs_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
