{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wrapper(learning_rate, dropout):\n",
    "\n",
    "\timport tensorflow as tf\n",
    "\timport numpy as np\n",
    "\tfrom hops import tensorboard\n",
    "\tfrom hops import hdfs\n",
    "\n",
    "\t# Training Parameters\n",
    "\t#learning_rate = 0.001\n",
    "\tnum_steps = 200\n",
    "\tbatch_size = 128\n",
    "\n",
    "\t# Network Parameters\n",
    "\tnum_input = 784 # MNIST data input (img shape: 28*28)\n",
    "\tnum_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\ttrain_filenames = [hdfs.project_path() + \"mnist/train.tfrecords\"]\n",
    "\tvalidation_filenames = [hdfs.project_path() + \"mnist/validation.tfrecords\"]\n",
    "\n",
    "\t# Create the neural network\n",
    "\tdef conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "\n",
    "\t    # Define a scope for reusing the variables\n",
    "\t    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "\t\t# TF Estimator input is a dict, in case of multiple inputs\n",
    "\t\tx = x_dict\n",
    "\n",
    "\t\t# MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "\t\t# Reshape to match picture format [Height x Width x Channel]\n",
    "\t\t# Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "\t\tx = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "\t\t# Convolution Layer with 32 filters and a kernel size of 5\n",
    "\t\tconv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "\t\t# Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "\t\tconv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "\t\t# Convolution Layer with 32 filters and a kernel size of 5\n",
    "\t\tconv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "\t\t# Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "\t\tconv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "\t\t# Flatten the data to a 1-D vector for the fully connected layer\n",
    "\t\tfc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "\t\t# Fully connected layer (in tf contrib folder for now)\n",
    "\t\tfc1 = tf.layers.dense(fc1, 1024)\n",
    "\t\t# Apply Dropout (if is_training is False, dropout is not applied)\n",
    "\t\tfc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "\t\t# Output layer, class prediction\n",
    "\t\tout = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "\t    return out\n",
    "\n",
    "\n",
    "\t# Define the model function (following TF Estimator Template)\n",
    "\tdef model_fn(features, labels, mode, params):\n",
    "\n",
    "\t    # Build the neural network\n",
    "\t    # Because Dropout have different behavior at training and prediction time, we\n",
    "\t    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "\t    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "\t    print logits_train\n",
    "\t    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "\n",
    "\n",
    "\t    # Predictions\n",
    "\t    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "\t    pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "\t    # If prediction mode, early return\n",
    "\t    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "\t    # Define loss and optimizer\n",
    "\t    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "\t\tlogits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "\t    lr = tf.train.exponential_decay(learning_rate, tf.train.get_global_step(),100000,0.96)\n",
    "\t    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\t    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "\t    # Evaluate the accuracy of the model\n",
    "\t    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "\t    image = tf.reshape(features[:10], [-1, 28, 28, 1])\n",
    "\t    tf.summary.image(\"image\", image)\n",
    "\t    # tf.summary.scalar('my_accuracy', acc_op[0])\n",
    "\n",
    "\t    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "\t    # the different ops for training, evaluating, ...\n",
    "\t    estim_specs = tf.estimator.EstimatorSpec(\n",
    "\t      mode=mode,\n",
    "\t      predictions=pred_classes,\n",
    "\t      loss=loss_op,\n",
    "\t      train_op=train_op,\n",
    "\t      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "\t    return estim_specs\n",
    "\n",
    "\n",
    "\tdef data_input_fn(filenames, batch_size=128, shuffle=False, repeat=None):\n",
    "\n",
    "\t    def parser(serialized_example):\n",
    "\t\t\"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "\t\tfeatures = tf.parse_single_example(\n",
    "\t\t    serialized_example,\n",
    "\t\t    features={\n",
    "\t\t        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "\t\t        'label': tf.FixedLenFeature([], tf.int64),\n",
    "\t\t    })\n",
    "\t\timage = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "\t\timage.set_shape([28 * 28])\n",
    "\n",
    "\t\t# Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "\t\timage = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "\t\tlabel = tf.cast(features['label'], tf.int32)\n",
    "\t\treturn image, label\n",
    "\n",
    "\t    def _input_fn():\n",
    "\t\t# Import MNIST data\n",
    "\t\tdataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "\t\t# Map the parser over dataset, and batch results by up to batch_size\n",
    "\t\tdataset = dataset.map(parser, num_threads=1, output_buffer_size=batch_size)\n",
    "\t\tif shuffle:\n",
    "\t\t    dataset = dataset.shuffle(buffer_size=128)\n",
    "\t\tdataset = dataset.batch(batch_size)\n",
    "\t\tdataset = dataset.repeat(repeat)\n",
    "\t\titerator = dataset.make_one_shot_iterator()\n",
    "\n",
    "\t\tfeatures, labels = iterator.get_next()\n",
    "\n",
    "\t\treturn features, labels\n",
    "\n",
    "\t    return _input_fn\n",
    "\n",
    "\n",
    "\trun_config = tf.contrib.learn.RunConfig(\n",
    "\t    model_dir=tensorboard.logdir(),\n",
    "\t    save_checkpoints_steps=10,\n",
    "\t    save_summary_steps=5,\n",
    "\t    log_step_count_steps=10)\n",
    "\n",
    "\thparams = tf.contrib.training.HParams(\n",
    "        learning_rate=learning_rate, dropout_rate=dropout)\n",
    "\n",
    "\tsummary_hook = tf.train.SummarySaverHook(\n",
    "\t      save_steps = run_config.save_summary_steps,\n",
    "\t      scaffold= tf.train.Scaffold(),\n",
    "\t      summary_op=tf.summary.merge_all())\n",
    "\n",
    "\tmnist_estimator = tf.estimator.Estimator(\n",
    "\t    model_fn=model_fn,\n",
    "\t    config=run_config,\n",
    "\t    params=hparams\n",
    "\t)\n",
    "\n",
    "\n",
    "\ttrain_input_fn = data_input_fn(train_filenames[0], batch_size=batch_size)\n",
    "\teval_input_fn = data_input_fn(validation_filenames[0], batch_size=batch_size)\n",
    "\n",
    "\texperiment = tf.contrib.learn.Experiment(\n",
    "\t    mnist_estimator,\n",
    "\t    train_input_fn=train_input_fn,\n",
    "\t    eval_input_fn=eval_input_fn,\n",
    "\t    train_steps=num_steps,\n",
    "\t    min_eval_frequency=5,\n",
    "\t    eval_hooks=[summary_hook]\n",
    "\t)\n",
    "\n",
    "\texperiment.train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.0005], 'dropout': [0.7]}"
     ]
    }
   ],
   "source": [
    "from hops import util\n",
    "\n",
    "#Define dict for hyperparameters\n",
    "args_dict = {'learning_rate': [0.0005], 'dropout': [0.7]}\n",
    "\n",
    "# Generate a grid for the given hyperparameters\n",
    "args_dict_grid = util.grid_params(args_dict)\n",
    "\n",
    "print(args_dict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished TensorFlow job \n",
      "\n",
      "Make sure to check /Logs/TensorFlow/application_1511276242554_0465/runId.0 for logfile and TensorBoard logdir\n",
      "Elapsed time: 132.429859877"
     ]
    }
   ],
   "source": [
    "from hops import tflauncher\n",
    "import timeit\n",
    " \n",
    "start_time = timeit.default_timer()\n",
    "tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper, args_dict_grid)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print \"Elapsed time: \" + str(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hops import tensorboard\n",
    "\n",
    "# Visualize all TensorBoard events for the jobs in the same TensorBoard\n",
    "tensorboard.visualize(spark, tensorboard_hdfs_logdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
