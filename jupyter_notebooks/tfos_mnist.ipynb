{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1145</td><td>application_1511276242554_0486</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1511276242554_0486/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop22:8042/node/containerlogs/container_e25_1511276242554_0486_01_000001/bonus_lab2__diegorc0\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "def mnist_fun(args, ctx):\n",
    "    \n",
    "    def print_log(worker_num, arg):\n",
    "        print(\"%d: \" %worker_num)\n",
    "        print(arg)\n",
    "\n",
    "    from tensorflowonspark import TFNode\n",
    "    from datetime import datetime\n",
    "    import getpass\n",
    "    import math\n",
    "    import numpy\n",
    "    import os\n",
    "    import signal\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "  \n",
    "    # Used to get TensorBoard logdir for TensorBoard that show up in HopsWorks\n",
    "    from hops import tensorboard\n",
    "\n",
    "    IMAGE_PIXELS=28\n",
    "    worker_num = ctx.worker_num\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "    cluster_spec = ctx.cluster_spec\n",
    "    num_workers = len(cluster_spec['worker'])\n",
    "\n",
    "    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n",
    "    if job_name == \"ps\":\n",
    "        time.sleep((worker_num + 1) * 5)\n",
    "\n",
    "    # Parameters\n",
    "    hidden_units = 128\n",
    "    batch_size   = 100\n",
    "\n",
    "    # Get TF cluster and server instances\n",
    "    cluster, server = TFNode.start_cluster_server(ctx, 1, args.rdma)\n",
    "\n",
    "    def read_tfr_examples(path, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "\n",
    "        # Setup queue of TFRecord filenames\n",
    "        tf_record_pattern = os.path.join(path, 'part-*')\n",
    "        files = tf.gfile.Glob(tf_record_pattern)\n",
    "        queue_name = \"file_queue\"\n",
    "\n",
    "        # split input files across workers, if specified\n",
    "        if task_index is not None and num_workers is not None:\n",
    "            num_files = len(files)\n",
    "            files = files[task_index:num_files:num_workers]\n",
    "            queue_name = \"file_queue_{0}\".format(task_index)\n",
    "\n",
    "        print_log(worker_num, \"files: {0}\".format(files))\n",
    "        file_queue = tf.train.string_input_producer(files, shuffle=False, capacity=1000, num_epochs=num_epochs, name=queue_name)\n",
    "\n",
    "        # Setup reader for examples\n",
    "        reader = tf.TFRecordReader(name=\"reader\")\n",
    "        _, serialized = reader.read(file_queue)\n",
    "        feature_def = {'label': tf.FixedLenFeature([10], tf.int64), 'image': tf.FixedLenFeature([784], tf.int64) }\n",
    "        features = tf.parse_single_example(serialized, feature_def)\n",
    "        norm = tf.constant(255, dtype=tf.float32, shape=(784,))\n",
    "        image = tf.div(tf.to_float(features['image']), norm)\n",
    "        print_log(worker_num, \"image: {0}\".format(image))\n",
    "        label = tf.to_float(features['label'])\n",
    "        print_log(worker_num, \"label: {0}\".format(label))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([image,label], batch_size, num_threads=args.readers, name=\"batch\")\n",
    "\n",
    "    def read_csv_examples(image_dir, label_dir, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "        # Setup queue of csv image filenames\n",
    "        tf_record_pattern = os.path.join(image_dir, 'part-*')\n",
    "        images = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"images: {0}\".format(images))\n",
    "        image_queue = tf.train.string_input_producer(images, shuffle=False, capacity=1000, num_epochs=num_epochs, name=\"image_queue\")\n",
    "\n",
    "        # Setup queue of csv label filenames\n",
    "        tf_record_pattern = os.path.join(label_dir, 'part-*')\n",
    "        labels = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"labels: {0}\".format(labels))\n",
    "        label_queue = tf.train.string_input_producer(labels, shuffle=False, capacity=1000, num_epochs=num_epochs, name=\"label_queue\")\n",
    "\n",
    "        # Setup reader for image queue\n",
    "        img_reader = tf.TextLineReader(name=\"img_reader\")\n",
    "        _, img_csv = img_reader.read(image_queue)\n",
    "        image_defaults = [ [1.0] for col in range(784) ]\n",
    "        img = tf.stack(tf.decode_csv(img_csv, image_defaults))\n",
    "        # Normalize values to [0,1]\n",
    "        norm = tf.constant(255, dtype=tf.float32, shape=(784,))\n",
    "        image = tf.div(img, norm)\n",
    "        print_log(worker_num, \"image: {0}\".format(image))\n",
    "\n",
    "        # Setup reader for label queue\n",
    "        label_reader = tf.TextLineReader(name=\"label_reader\")\n",
    "        _, label_csv = label_reader.read(label_queue)\n",
    "        label_defaults = [ [1.0] for col in range(10) ]\n",
    "        label = tf.stack(tf.decode_csv(label_csv, label_defaults))\n",
    "        print_log(worker_num, \"label: {0}\".format(label))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([image,label], batch_size, num_threads=args.readers, name=\"batch_csv\")\n",
    "\n",
    "    if job_name == \"ps\":\n",
    "        server.join()\n",
    "    elif job_name == \"worker\":\n",
    "        # Assigns ops to the local worker by default.\n",
    "        with tf.device(tf.train.replica_device_setter(\n",
    "            worker_device=\"/job:worker/task:%d\" % task_index,\n",
    "            cluster=cluster)):\n",
    "\n",
    "            # Variables of the hidden layer\n",
    "            hid_w = tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units],\n",
    "                                  stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\n",
    "            hid_b = tf.Variable(tf.zeros([hidden_units]), name=\"hid_b\")\n",
    "            tf.summary.histogram(\"hidden_weights\", hid_w)\n",
    "\n",
    "            # Variables of the softmax layer\n",
    "            sm_w = tf.Variable(tf.truncated_normal([hidden_units, 10],\n",
    "                                  stddev=1.0 / math.sqrt(hidden_units)), name=\"sm_w\")\n",
    "            sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n",
    "            tf.summary.histogram(\"softmax_weights\", sm_w)\n",
    "\n",
    "            # Placeholders or QueueRunner/Readers for input data\n",
    "            num_epochs = 1 if args.mode == \"inference\" else None if args.epochs == 0 else args.epochs\n",
    "            index = task_index if args.mode == \"inference\" else None\n",
    "            workers = num_workers if args.mode == \"inference\" else None\n",
    "\n",
    "            if args.format == \"csv\":\n",
    "                images = TFNode.hdfs_path(ctx, args.images)\n",
    "                labels = TFNode.hdfs_path(ctx, args.labels)\n",
    "                x, y_ = read_csv_examples(images, labels, 100, num_epochs, index, workers)\n",
    "            elif args.format == \"tfr\":\n",
    "                images = TFNode.hdfs_path(ctx, args.images)\n",
    "                x, y_ = read_tfr_examples(images, 100, num_epochs, index, workers)\n",
    "            else:\n",
    "                raise(\"{0} format not supported for tf input mode\".format(args.format))\n",
    "\n",
    "            x_img = tf.reshape(x, [-1, IMAGE_PIXELS, IMAGE_PIXELS, 1])\n",
    "            tf.summary.image(\"x_img\", x_img)\n",
    "\n",
    "            hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "            hid = tf.nn.relu(hid_lin)\n",
    "\n",
    "            y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "\n",
    "            global_step = tf.Variable(0)\n",
    "\n",
    "            loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "            train_op = tf.train.AdagradOptimizer(0.01).minimize(\n",
    "              loss, global_step=global_step)\n",
    "\n",
    "            # Test trained model\n",
    "            label = tf.argmax(y_, 1, name=\"label\")\n",
    "            prediction = tf.argmax(y, 1,name=\"prediction\")\n",
    "            correct_prediction = tf.equal(prediction, label)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "            tf.summary.scalar(\"acc\", accuracy)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "            # Create a \"supervisor\", which oversees the training process and stores model state into HDFS\n",
    "            logdir = tensorboard.logdir()\n",
    "            print(\"tensorflow model path: {0}\".format(logdir))\n",
    "\n",
    "            if job_name == \"worker\" and task_index == 0:\n",
    "                summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "            if args.mode == \"train\":\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                       logdir=logdir,\n",
    "                                       init_op=init_op,\n",
    "                                       summary_op=None,\n",
    "                                       summary_writer=None,\n",
    "                                       saver=saver,\n",
    "                                       global_step=global_step,\n",
    "                                       stop_grace_secs=300,\n",
    "                                       save_model_secs=10)\n",
    "            else:\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                       logdir=logdir,\n",
    "                                       summary_op=None,\n",
    "                                       saver=saver,\n",
    "                                       global_step=global_step,\n",
    "                                       stop_grace_secs=300,\n",
    "                                       save_model_secs=0)\n",
    "            output_dir = TFNode.hdfs_path(ctx, args.output)\n",
    "            output_file = tf.gfile.Open(\"{0}/part-{1:05d}\".format(output_dir, worker_num), mode='w')\n",
    "\n",
    "          # The supervisor takes care of session initialization, restoring from\n",
    "          # a checkpoint, and closing when done or an error occurs.\n",
    "    with sv.managed_session(server.target) as sess:\n",
    "        print(\"{0} session ready\".format(datetime.now().isoformat()))\n",
    "\n",
    "        # Loop until the supervisor shuts down or 1000000 steps have completed.\n",
    "        step = 0\n",
    "        count = 0\n",
    "        while not sv.should_stop() and step < args.steps:\n",
    "        # Run a training step asynchronously.\n",
    "        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n",
    "        # perform *synchronous* training.\n",
    "\n",
    "            # using QueueRunners/Readers\n",
    "            if args.mode == \"train\":\n",
    "                if (step % 100 == 0):\n",
    "                    print(\"{0} step: {1} accuracy: {2}\".format(datetime.now().isoformat(), step, sess.run(accuracy)))\n",
    "                _, summary, step = sess.run([train_op, summary_op, global_step])\n",
    "                if sv.is_chief:\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "            else: # args.mode == \"inference\"\n",
    "                labels, pred, acc = sess.run([label, prediction, accuracy])\n",
    "                #print(\"label: {0}, pred: {1}\".format(labels, pred))\n",
    "                print(\"acc: {0}\".format(acc))\n",
    "                for i in range(len(labels)):\n",
    "                    count += 1\n",
    "                    output_file.write(\"{0} {1}\\n\".format(labels[i], pred[i]))\n",
    "                print(\"count: {0}\".format(count))\n",
    "\n",
    "        if args.mode == \"inference\":\n",
    "            output_file.close()\n",
    "            # Delay chief worker from shutting down supervisor during inference, since it can load model, start session,\n",
    "            # run inference and request stop before the other workers even start/sync their sessions.\n",
    "        if task_index == 0:\n",
    "            time.sleep(60)\n",
    "\n",
    "        # Ask for all the services to stop.\n",
    "        print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n",
    "        sv.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_executors\n",
      "10\n",
      "('args:', Namespace(cluster_size=10, epochs=0, format='csv', images='/Projects/bonus_lab2/mnist/train/images', labels='/Projects/bonus_lab2/mnist/train/labels', mode='train', model='mnist_model', output='predictions', rdma=False, readers=1, steps=200, tensorboard=False))\n",
      "2017-11-29T00:45:02.182603 ===== Start\n",
      "2017-11-29T00:46:56.582516 ===== Stop"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from hops import util\n",
    "from hops import hdfs\n",
    "\n",
    "from tensorflowonspark import TFCluster\n",
    "\n",
    "sc = spark.sparkContext\n",
    "num_executors = util.num_executors(spark)\n",
    "print(\"num_executors\")\n",
    "print(num_executors)\n",
    "num_ps = util.num_param_servers(spark)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-e\", \"--epochs\", help=\"number of epochs\", type=int, default=0)\n",
    "parser.add_argument(\"-f\", \"--format\", help=\"example format: (csv|pickle|tfr)\", choices=[\"csv\",\"pickle\",\"tfr\"], default=\"csv\")\n",
    "parser.add_argument(\"-i\", \"--images\", help=\"HDFS path to MNIST images in parallelized format\", default='/Projects/' + hdfs.project_name() + '/mnist/train/images')\n",
    "parser.add_argument(\"-l\", \"--labels\", help=\"HDFS path to MNIST labels in parallelized format\", default = '/Projects/' + hdfs.project_name() + '/mnist/train/labels')\n",
    "parser.add_argument(\"-m\", \"--model\", help=\"HDFS path to save/load model during train/test\", default=\"mnist_model\")\n",
    "parser.add_argument(\"-n\", \"--cluster_size\", help=\"number of nodes in the cluster (for Spark Standalone)\", type=int, default=num_executors)\n",
    "parser.add_argument(\"-o\", \"--output\", help=\"HDFS path to save test/inference output\", default=\"predictions\")\n",
    "parser.add_argument(\"-r\", \"--readers\", help=\"number of reader/enqueue threads\", type=int, default=1)\n",
    "parser.add_argument(\"-s\", \"--steps\", help=\"maximum number of steps\", type=int, default=200)\n",
    "parser.add_argument(\"-tb\", \"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\n",
    "parser.add_argument(\"-X\", \"--mode\", help=\"train|inference\", default=\"train\")\n",
    "parser.add_argument(\"-c\", \"--rdma\", help=\"use rdma connection\", default=False)\n",
    "args = parser.parse_args()\n",
    "print(\"args:\",args)\n",
    "\n",
    "\n",
    "print(\"{0} ===== Start\".format(datetime.now().isoformat()))\n",
    "cluster = TFCluster.run(sc, mnist_fun, args, args.cluster_size, num_ps, args.tensorboard, TFCluster.InputMode.TENSORFLOW)\n",
    "cluster.shutdown()\n",
    "print(\"{0} ===== Stop\".format(datetime.now().isoformat()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_executors\n",
    "10\n",
    "\n",
    "('args:', Namespace(cluster_size=10, epochs=0, format='csv', images='/Projects/bonus_lab2/mnist/train/images', labels='/Projects/bonus_lab2/mnist/train/labels', mode='train', model='mnist_model', output='predictions', rdma=False, readers=1, steps=200, tensorboard=False))\n",
    "\n",
    "2017-11-29T00:45:02.182603 ===== Start\n",
    "\n",
    "2017-11-29T00:46:56.582516 ===== Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "num_executors\n",
    "6\n",
    "\n",
    "('args:', Namespace(cluster_size=6, epochs=0, format='csv', images='/Projects/bonus_lab2/mnist/train/images', labels='/Projects/bonus_lab2/mnist/train/labels', mode='train', model='mnist_model', output='predictions', rdma=False, readers=1, steps=200, tensorboard=False))\n",
    "\n",
    "2017-11-28T23:26:02.923169 ===== Start\n",
    "\n",
    "2017-11-28T23:28:23.247097 ===== Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "num_executors\n",
    "2\n",
    "\n",
    "('args:', Namespace(cluster_size=2, epochs=0, format='csv', images='/Projects/bonus_lab2/mnist/train/images', labels='/Projects/bonus_lab2/mnist/train/labels', mode='train', model='mnist_model', output='predictions', rdma=False, readers=1, steps=200, tensorboard=False))\n",
    "\n",
    "2017-11-28T23:32:09.808702 ===== Start\n",
    "\n",
    "2017-11-28T23:35:15.368340 ===== Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('args:', Namespace(cluster_size=1, epochs=0, format='csv', images='/Projects/demo_tensorflow_diegorc1/mnist/train/images', labels='/Projects/demo_tensorflow_diegorc1/mnist/train/labels', mode='train', model='mnist_model', output='predictions', rdma=False, readers=1, steps=1000, tensorboard=False))"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
